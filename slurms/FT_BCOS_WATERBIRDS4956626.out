============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius+and+Lisa#SoftwarepolicySnelliusandLisa-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
Train data size: 4796
Annotated: 47, Total: 4796
  0%|          | 0/350 [00:00<?, ?it/s]
  0%|          | 0/75 [00:00<?, ?it/s][A/home/scur1040/.conda/envs/FACTifAI/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,

  1%|â–         | 1/75 [00:01<01:44,  1.41s/it][A  1%|â–         | 1/75 [00:01<02:01,  1.64s/it]
  0%|          | 0/350 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/gpfs/home4/scur1040/Fact_AI/FACTifAI_2024_3/train.py", line 727, in <module>
    main(args)
  File "/gpfs/home4/scur1040/Fact_AI/FACTifAI_2024_3/train.py", line 444, in main
    attributions = attributor(features, logits, classes=gt_classes).squeeze(
  File "/gpfs/home4/scur1040/Fact_AI/FACTifAI_2024_3/attribution_methods.py", line 45, in __call__
    return self._call_batch_mode(feature, output, classes)
  File "/gpfs/home4/scur1040/Fact_AI/FACTifAI_2024_3/attribution_methods.py", line 106, in _call_batch_mode
    grads = torch.autograd.grad(
  File "/home/scur1040/.conda/envs/FACTifAI/lib/python3.10/site-packages/torch/autograd/__init__.py", line 394, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 19.56 MiB is free. Including non-PyTorch memory, this process has 39.36 GiB memory in use. Of the allocated memory 38.38 GiB is allocated by PyTorch, and 484.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Train data size: 4796
Annotated: 47, Total: 4796
  0%|          | 0/350 [00:00<?, ?it/s]
  0%|          | 0/75 [00:00<?, ?it/s][A/home/scur1040/.conda/envs/FACTifAI/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,

  1%|â–         | 1/75 [00:00<01:04,  1.15it/s][A  1%|â–         | 1/75 [00:01<01:18,  1.05s/it]
  0%|          | 0/350 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/gpfs/home4/scur1040/Fact_AI/FACTifAI_2024_3/train.py", line 727, in <module>
    main(args)
  File "/gpfs/home4/scur1040/Fact_AI/FACTifAI_2024_3/train.py", line 444, in main
    attributions = attributor(features, logits, classes=gt_classes).squeeze(
  File "/gpfs/home4/scur1040/Fact_AI/FACTifAI_2024_3/attribution_methods.py", line 45, in __call__
    return self._call_batch_mode(feature, output, classes)
  File "/gpfs/home4/scur1040/Fact_AI/FACTifAI_2024_3/attribution_methods.py", line 106, in _call_batch_mode
    grads = torch.autograd.grad(
  File "/home/scur1040/.conda/envs/FACTifAI/lib/python3.10/site-packages/torch/autograd/__init__.py", line 394, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 185.56 MiB is free. Including non-PyTorch memory, this process has 39.20 GiB memory in use. Of the allocated memory 38.22 GiB is allocated by PyTorch, and 479.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

JOB STATISTICS
==============
Job ID: 4956626
Cluster: snellius
User/Group: scur1040/scur1040
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:02:05
CPU Efficiency: 19.84% of 00:10:30 core-walltime
Job Wall-clock time: 00:00:35
Memory Utilized: 1.16 MB
Memory Efficiency: 0.00% of 120.00 GB
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
